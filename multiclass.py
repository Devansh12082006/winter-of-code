# -*- coding: utf-8 -*-
"""multiclass.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U7ewUHT5eKQOzwcS8M35bryvcda5N7is
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

a=pd.read_csv('/content/multi_classification_train.csv')
x_train=a.values[0:24000,1:21]
y_train=a.values[0:24000,21:22]

x_test=a.values[24000:48000,1:21]
y_test=a.values[24000:48000,21:22]

def softmax(z,y):
  # TAKEN Z=J,1 MATRIX
  g=np.zeros(z.shape)
  n=z.shape[0]
  for i in range(n):
    g[i]=np.exp(z[i])/(np.sum(np.exp(z)))
  return g

def relu(z):
  g=np.max(z,0)
  return g

def relu_derivative(Z):
    return Z > 0

def layer(a_in,w,b):
  # hare take b=1Xj
  a_out=np.matmul(a_in,w)
  a_out=np.sum(a_out,axis=0,keepdims=true)
  a_out=a_out+b
# here a_out is 1Xj matrix where j is no. of neurons in layer
  return a_out

def forward_propagation(X,W1,b1,W2,b2,W3,b3):
  Z1=layer(X,W1,b1)
  A1=relu(Z1)
  Z2=layer(A1,W2,b2)
  A2=relu(Z2)
  Z3=layer(A2,W3,b3)
  A3=softmax(Z3.T)
  return A3,Z1,Z2,A1,A2,Z3

def loss(z,y):
  g=softmax(z)
  loss=np.zeros(g.shape)
  for j in range(g.shape[0]):
    if y==j:
      loss[j]=-np.log(g[j])
  return loss

def initialize_weights(input_size, hidden_size, output_size):
    W1 = np.random.randn(input_size, hidden_size)
    b1 = np.zeros((1, hidden_size))
    W2 = np.random.randn(hidden_size, hidden_size)   # Second hidden layer weights
    b2 = np.zeros((1, hidden_size))  # Bias for second hidden layer
    W3 = np.random.randn(hidden_size, output_size)  # Output layer weights
    b3 = np.zeros((1, output_size))  # Bias for output layer
    return W1, b1, W2, b2, W3, b3

def backward_propagation(X, Y, cache, W1, W2, W3):
    Z1, A1, Z2, A2, Z3, A3 = cache
    m = X.shape[0]

    dZ3 = A3 - Y
    dW3 = np.dot(A2.T, dZ3) / m
    db3 = np.sum(dZ3, axis=0, keepdims=True) / m

    dA2 = np.dot(dZ3, W3.T)
    dZ2 = dA2 * relu_derivative(Z2)
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * relu_derivative(Z1)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    return dW1, db1, dW2, db2, dW3, db3

def update_weights(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W3 -= learning_rate * dW3
    b3 -= learning_rate * db3
    return W1, b1, W2, b2, W3, b3

def compute_loss(Y, A3):
    m = Y.shape[0]
    return -np.sum(Y * np.log(A3)) / m

def train(X, Y, W1, b1, W2, b2, W3, b3, epochs, learning_rate):
    W1, b1, W2, b2, W3, b3 = initialize_weights(input_size, hidden_size, output_size)
    losses = []

    for epoch in range(epochs):
        A3, cache = forward_propagation(X, W1, b1, W2, b2, W3, b3)
        loss = compute_loss(Y, A3)
        dW1, db1, dW2, db2, dW3, db3 = backward_propagation(X, Y, cache, W1, W2, W3)
        W1, b1, W2, b2, W3, b3 = update_weights(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)

        losses.append(loss)
        if epoch % 10 == 0:
            print(f'Epoch {epoch}, Loss: {loss:.4f}')

    return W1, b1, W2, b2, W3, b3, losses

def predict(X, W1, b1, W2, b2, W3, b3):
    A3, _ = forward_propagation(X, W1, b1, W2, b2, W3, b3)
    return np.argmax(A3, axis=1)

input_size = x_train.shape[1]
hidden_size = 64
epochs = 200
learning_rate = 0.1

W1, b1, W2, b2, W3, b3, losses = train(x_train, y_train, epochs = 200, learning_rate=0.1)

print(W1, b1, W2, b2, W3, b3, losses)

def compute_f1_score(y_true, y_pred):

    tp = np.zeros(y_true.shape[0])
    fp = np.zeros(y_true.shape[0])
    fn = np.zeros(y_true.shape[0])

    for i in range(y_true.shape[0]):
        if y_true[i] == y_pred[i]:
            tp[i] = 1
        elif y_pred[i] != y_true[i]:
            fp[i] = 1
            fn[i] = 1

    precision = np.sum(tp) / (np.sum(tp) + np.sum(fp)) if np.sum(tp) + np.sum(fp) != 0 else 0
    recall = np.sum(tp) / (np.sum(tp) + np.sum(fn)) if np.sum(tp) + np.sum(fn) != 0 else 0

    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0

    return f1
y_pred = predict(x_test, W1, b1, W2, b2, W3, b3)

y_test_labels = np.argmax(y_test, axis=1)

f1 = compute_f1_score(y_test_labels, y_pred)
print(f'Test F1 Score: {f1:.2f}')

